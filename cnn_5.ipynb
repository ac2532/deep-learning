{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "#imports for problem 2\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import json\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "#imports for problem 3\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "#imports for problem 5\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Vision Transformers\n",
    "Instead of transfer learning with a CNN, I did a transfer learning with a Vision Transformer. I identified a strong vision transformer architecture for transfer learning that was pre-trained on ImageNet-1K (note that some pre-trained models are pre-trained on larger datasets). We suggest using Swin Transformers or later incarnations of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Trainset size: 3680\n",
      "- Testset size: 3669\n",
      "- Number of classes: 37\n",
      "- Classes: ['Abyssinian', 'American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Bengal', 'Birman', 'Bombay', 'Boxer', 'British Shorthair', 'Chihuahua', 'Egyptian Mau', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees', 'Havanese', 'Japanese Chin', 'Keeshond', 'Leonberger', 'Maine Coon', 'Miniature Pinscher', 'Newfoundland', 'Persian', 'Pomeranian', 'Pug', 'Ragdoll', 'Russian Blue', 'Saint Bernard', 'Samoyed', 'Scottish Terrier', 'Shiba Inu', 'Siamese', 'Sphynx', 'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aya/opt/anaconda3/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth\" to /Users/aya/.cache/torch/hub/checkpoints/swin_base_patch4_window7_224_22kto1k.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "Linear(in_features=1024, out_features=1000, bias=True)\n",
      "Evaluation mode set\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "#dataset and data loaders\n",
    "train = torchvision.datasets.OxfordIIITPet(root='./data', split='trainval', transform=transform, download=True)\n",
    "test = torchvision.datasets.OxfordIIITPet(root='./data', split='test', transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f'- Trainset size: {len(train)}')\n",
    "print(f'- Testset size: {len(test)}')\n",
    "print(f'- Number of classes: {len(train.classes)}')\n",
    "print(f'- Classes: {train.classes}')\n",
    "\n",
    "#load pre-trained model, remove last fully connected layer and set to evaluation mode\n",
    "model_name = 'swin_tiny_patch4_window7_224'\n",
    "num_classes = 1000\n",
    "pretrained = True\n",
    "model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "print(list(model.children())[-2])\n",
    "print(list(model.children())[-1])\n",
    "\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model.eval()\n",
    "print('Evaluation mode set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features for training \n",
    "train_features = []\n",
    "train_labels = []\n",
    "for images, labels in train_loader:\n",
    "    features = model(images)\n",
    "    features = features.view(features.size(0), -1)\n",
    "    train_features.append(features.detach().numpy())\n",
    "    train_labels.append(labels.detach().numpy())\n",
    "train_features = np.concatenate(train_features, axis=0)\n",
    "train_labels = np.concatenate(train_labels, axis=0)\n",
    "\n",
    "#extract features for test \n",
    "test_features = []\n",
    "test_labels = []\n",
    "for images, labels in test_loader:\n",
    "    features = model(images)\n",
    "    features = features.view(features.size(0), -1)\n",
    "    test_features.append(features.detach().numpy())\n",
    "    test_labels.append(labels.detach().numpy())\n",
    "test_features = np.concatenate(test_features, axis=0)\n",
    "test_labels = np.concatenate(test_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (3680, 50176)\n",
      "Train labels shape: (3680,)\n",
      "Test features shape: (3669, 50176)\n",
      "Test labels shape: (3669,)\n",
      "Predicted labels shape: (3669,)\n",
      "Overall accuracy: 0.938\n",
      "Mean per class accuracy: 0.938\n",
      "Mean-per-class accuracy using sklearn: 0.938\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "#normalize train and test features\n",
    "train_features_norm = np.linalg.norm(train_features, axis=1, keepdims=True)\n",
    "train_features = train_features / train_features_norm\n",
    "test_features_norm = np.linalg.norm(test_features, axis=1, keepdims=True)\n",
    "test_features = test_features / test_features_norm\n",
    "\n",
    "#train logistic regression classifier and predict classes for test data\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(train_features, train_labels)\n",
    "predicted_labels = classifier.predict(test_features)\n",
    "\n",
    "print(f\"Predicted labels shape: {predicted_labels.shape}\")\n",
    "\n",
    "#compute overall accuracy\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(f\"Overall accuracy: {accuracy:.3f}\")\n",
    "#compute mean per class accuracy\n",
    "class_accuracy = []\n",
    "for i in range(len(train.classes)):\n",
    "    class_accuracy.append(accuracy_score(test_labels[test_labels==i], predicted_labels[test_labels==i]))\n",
    "print(f\"Mean per class accuracy: {np.mean(class_accuracy):.3f}\")\n",
    "#computing mean-per-class accuracy using sklearn\n",
    "accuracy = balanced_accuracy_score(test_labels, predicted_labels)\n",
    "print(f\"Mean-per-class accuracy using sklearn: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of each classes accuracies: \n",
      "[0.9285714285714286, 0.95, 0.72, 0.98, 0.98, 0.96, 0.89, 0.9886363636363636, 0.9191919191919192, 0.74, 0.96, 0.7835051546391752, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9797979797979798, 1.0, 0.83, 0.94, 1.0, 0.93, 0.99, 0.99, 0.75, 0.84, 0.99, 1.0, 1.0, 0.99, 0.95, 0.99, 0.7865168539325843, 0.96, 0.99]\n",
      "corresponding to: \n",
      "['Abyssinian', 'American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Bengal', 'Birman', 'Bombay', 'Boxer', 'British Shorthair', 'Chihuahua', 'Egyptian Mau', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees', 'Havanese', 'Japanese Chin', 'Keeshond', 'Leonberger', 'Maine Coon', 'Miniature Pinscher', 'Newfoundland', 'Persian', 'Pomeranian', 'Pug', 'Ragdoll', 'Russian Blue', 'Saint Bernard', 'Samoyed', 'Scottish Terrier', 'Shiba Inu', 'Siamese', 'Sphynx', 'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n"
     ]
    }
   ],
   "source": [
    "print(f'List of each classes accuracies: \\n{class_accuracy}')\n",
    "classes = train.classes\n",
    "print(f'corresponding to: \\n{classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a Vision Transformer (Swin Transformer) gives a better performance for the following classes: \n",
      " ['Abyssinian', 'American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Bengal', 'Birman', 'Bombay', 'Chihuahua', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees', 'Havanese', 'Japanese Chin', 'Maine Coon', 'Miniature Pinscher', 'Newfoundland', 'Persian', 'Pomeranian', 'Pug', 'Ragdoll', 'Russian Blue', 'Scottish Terrier', 'Siamese', 'Sphynx', 'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n",
      "Using a ResNet18 instead gives a better performance for the following classes: \n",
      " ['Boxer', 'British Shorthair', 'Egyptian Mau', 'Keeshond']\n",
      "The two methods have the same accuracy for the following classes: \n",
      " ['Leonberger', 'Saint Bernard', 'Samoyed', 'Shiba Inu']\n"
     ]
    }
   ],
   "source": [
    "#copy pasting mean per class accuracies found in problem 3\n",
    "class_acc_pb3 = [0.826530612244898, 0.91, 0.52, 0.9, 0.96, 0.88, 0.82, 0.9318181818181818, 0.9494949494949495, 0.77, 0.91, \n",
    "0.8041237113402062, 0.92, 0.94, 0.98, 0.97, 0.98, 0.96, 1.0, 1.0, 0.82, 0.87, 0.98, 0.89, 0.93, 0.93, 0.74, 0.83, 0.99, 1.0, \n",
    "0.98989898989899, 0.99, 0.89, 0.91, 0.5393258426966292, 0.91, 0.95]\n",
    "n = len(class_accuracy)\n",
    "best_vt, best_cnn, same_perf = [], [], []\n",
    "for i in range(n):\n",
    "    if class_accuracy[i] > class_acc_pb3[i]:\n",
    "        best_vt.append(classes[i])\n",
    "    elif class_accuracy[i] < class_acc_pb3[i]:\n",
    "        best_cnn.append(classes[i])\n",
    "    elif class_accuracy[i] == class_acc_pb3[i]:\n",
    "        same_perf.append(classes[i])\n",
    "\n",
    "print(f'Using a Vision Transformer (Swin Transformer) gives a better performance for the following classes: \\n {best_vt}')\n",
    "print(f'Using a ResNet18 instead gives a better performance for the following classes: \\n {best_cnn}')\n",
    "print(f'The two methods have the same accuracy for the following classes: \\n {same_perf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Briefly discuss the Vision Transformer you selected. \n",
    "\n",
    "As we are handling an image classification task, I have used Swin Transformer. It is an improved version of the original Vision Transformer model for image classification tasks. \n",
    "It 'builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window' (https://paperswithcode.com/method/swin-transformer). In other words, its hierarchical design can efficiently handle large images by dividing them into smaller patches and processing them with a window-based mechanism. This allows the transformer to capture both local and global contextual information from the images while reducing computational cost and memory requirement, explaining its improved accuracy and ability to generalize well.\n",
    "Moreover, it has has been pretrained on ImageNet-1K and has shown good performance, performing better than other models like CNNs (which we have proved above) and other transformer-based architectures.\n",
    "\n",
    "- Compare performance of the Vision Transformer to the CNN you used earlier in terms of overall performance. \n",
    "\n",
    "The Swin transformer has achieved an overall performance of 93.8% compared to 89.4% for the CNN previously used (resnet-18) which is a better performance as expected (+3.4%). Its hierarchical design and ability to capture both local and global contextual information as stated before are probably the reasons to this higher accuracy.\n",
    "\n",
    "- Are there some images that the CNN gets correct but the Swin Transformer misclassifies, and vice versa?\n",
    "\n",
    "Using a Vision Transformer (Swin Transformer) gives a better performance for the following classes: \n",
    " ['Abyssinian', 'American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Bengal', 'Birman', 'Bombay', 'Chihuahua', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees', 'Havanese', 'Japanese Chin', 'Maine Coon', 'Miniature Pinscher', 'Newfoundland', 'Persian', 'Pomeranian', 'Pug', 'Ragdoll', 'Russian Blue', 'Scottish Terrier', 'Siamese', 'Sphynx', 'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n",
    "Using a ResNet18 instead gives a better performance for the following classes: \n",
    " ['Boxer', 'British Shorthair', 'Egyptian Mau', 'Keeshond']\n",
    "The two methods have the same accuracy for the following classes: \n",
    " ['Leonberger', 'Saint Bernard', 'Samoyed', 'Shiba Inu']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sources:\n",
    "- https://pytorch.org/vision/stable/models.html\n",
    "- ChatGPT, StackExchange, StackOverflow\n",
    "- https://learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/ \n",
    "- https://www.kaggle.com/code/leifuer/intro-to-pytorch-loading-image-data\n",
    "- https://wandb.ai/shweta/Activation%20Functions/reports/Activation-Functions-Compared-With-Experiments--VmlldzoxMDQwOTQ#the-mish-activation-function\n",
    "- In-class MNIST Tutorial (google colab), CIFAR Tutorial\n",
    "- https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm\n",
    "- https://medium.com/pythoneers/vision-transformers-an-innovative-approach-to-image-processing-3387c398d67f\n",
    "- https://paperswithcode.com/method/swin-transformer\n",
    "- https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
